import json
from fnmatch import fnmatch
from pathlib import Path
from typing import Any, NamedTuple, cast

import einops
import numpy as np
import torch
from huggingface_hub import snapshot_download
from natsort import natsorted
from safetensors.torch import safe_open, save_model
from torch import Tensor, nn

from .config import SaeConfig
from .utils import decoder_impl


class EncoderOutput(NamedTuple):
    feature_acts: Tensor
    """SAE features after ReLU/JumpReLU"""

    top_acts: Tensor | None
    """Activations of the top-k latents."""

    top_indices: Tensor | None
    """Indices of the top-k features."""


class ForwardOutput(NamedTuple):
    sae_out: Tensor

    feature_acts: Tensor
    """SAE features after ReLU/JumpReLU"""

    topk_acts: Tensor | None
    """Activations of the top-k latents."""

    topk_indices: Tensor | None
    """Indices of the top-k features."""

    fvu: Tensor
    """Fraction of variance unexplained."""

    auxk_loss: Tensor
    """AuxK loss, if applicable."""

    multi_topk_fvu: Tensor
    """Multi-TopK FVU, if applicable."""

    l1_loss: Tensor
    """L1 loss for JumpReLU architectures"""

    l2_loss: Tensor
    """L2 loss over the reconstruction"""


def rectangle(x: torch.Tensor) -> torch.Tensor:
    return ((x > -0.5) & (x < 0.5)).to(x)


class Step(torch.autograd.Function):
    @staticmethod
    def forward(x: torch.Tensor, threshold: torch.Tensor, bandwidth: float) -> torch.Tensor:
        return (x > threshold).to(x)

    @staticmethod
    def setup_context(
        ctx: Any, inputs: tuple[torch.Tensor, torch.Tensor, float], output: torch.Tensor
    ) -> None:
        x, threshold, bandwidth = inputs
        del output
        ctx.save_for_backward(x, threshold)
        ctx.bandwidth = bandwidth

    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, None]:  # type: ignore[override]
        x, threshold = ctx.saved_tensors
        x_grad = 0.0 * grad_output
        bandwidth = ctx.bandwidth
        threshold_grad = torch.sum(
            -(1.0 / bandwidth) * rectangle((x - threshold) / bandwidth) * grad_output,
            dim=0,
        )
        return x_grad, threshold_grad, None


class JumpReLU(torch.autograd.Function):
    @staticmethod
    def forward(x: torch.Tensor, threshold: torch.Tensor, bandwidth: float) -> torch.Tensor:
        return (x * (x > threshold)).to(x)

    @staticmethod
    def setup_context(
        ctx: Any, inputs: tuple[torch.Tensor, torch.Tensor, float], output: torch.Tensor
    ) -> None:
        x, threshold, bandwidth = inputs
        del output
        ctx.save_for_backward(x, threshold)
        ctx.bandwidth = bandwidth

    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, None]:  # type: ignore[override]
        x, threshold = ctx.saved_tensors
        bandwidth = ctx.bandwidth
        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input
        threshold_grad = torch.sum(
            -(threshold / bandwidth) * rectangle((x - threshold) / bandwidth) * grad_output,
            dim=0,
        )
        return x_grad, threshold_grad, None


class Sae(nn.Module):
    def __init__(
        self,
        d_in: int,
        cfg: SaeConfig,
        device: str | torch.device = "cpu",
        dtype: torch.dtype | None = None,
        *,
        decoder: bool = True,
    ):
        super().__init__()
        if cfg.k > 0 and cfg.jumprelu:
            raise ValueError("JumpReLU is only supported for k <= 0.")
        self.cfg = cfg
        self.d_in = d_in
        self.num_latents = cfg.num_latents or d_in * cfg.expansion_factor

        self.encoder = nn.Linear(d_in, self.num_latents, device=device, dtype=dtype)
        torch.nn.init.kaiming_uniform_(self.encoder.weight.data)
        torch.nn.init.zeros_(self.encoder.bias.data)

        self.W_dec = (
            nn.Parameter(
                torch.nn.init.kaiming_uniform_(
                    torch.empty(self.num_latents, d_in, dtype=self.dtype, device=self.device)
                )
            )
            if decoder
            else None
        )
        self.b_dec = nn.Parameter(torch.zeros(d_in, dtype=dtype, device=device))

        if self.W_dec is not None and cfg.init_enc_as_dec_transpose:
            self.encoder.weight.data = self.W_dec.data.clone()

        if decoder and self.cfg.normalize_decoder:
            self.set_decoder_norm_to_unit_norm()

        self.jumprelu = self.cfg.jumprelu
        if self.jumprelu:
            self.log_threshold = nn.Parameter(
                torch.ones(self.num_latents, dtype=dtype, device=device)
                * np.log(cfg.jumprelu_init_threshold)
            )
            self.bandwidth = cfg.jumprelu_bandwidth

    @staticmethod
    def load_many(
        name: str,
        local: bool = False,
        layers: list[str] | None = None,
        device: str | torch.device = "cpu",
        *,
        decoder: bool = True,
        pattern: str | None = None,
    ) -> dict[str, "Sae"]:
        """Load SAEs for multiple hookpoints on a single model and dataset."""
        pattern = pattern + "/*" if pattern is not None else None
        if local:
            repo_path = Path(name)
        else:
            repo_path = Path(snapshot_download(name, allow_patterns=pattern))

        if layers is not None:
            return {
                layer: Sae.load_from_disk(repo_path / layer, device=device, decoder=decoder)
                for layer in natsorted(layers)
            }
        files = [
            f
            for f in repo_path.iterdir()
            if f.is_dir() and (pattern is None or fnmatch(f.name, pattern))
        ]
        return {
            f.name: Sae.load_from_disk(f, device=device, decoder=decoder)
            for f in natsorted(files, key=lambda f: f.name)
        }

    @staticmethod
    def load_from_hub(
        name: str,
        hookpoint: str | None = None,
        device: str | torch.device = "cpu",
        *,
        decoder: bool = True,
    ) -> "Sae":
        # Download from the HuggingFace Hub
        repo_path = Path(
            snapshot_download(
                name,
                allow_patterns=f"{hookpoint}/*" if hookpoint is not None else None,
            )
        )
        if hookpoint is not None:
            repo_path = repo_path / hookpoint

        # No layer specified, and there are multiple layers
        elif not repo_path.joinpath("cfg.json").exists():
            raise FileNotFoundError("No config file found; try specifying a layer.")

        return Sae.load_from_disk(repo_path, device=device, decoder=decoder)

    @staticmethod
    def load_from_disk(
        path: Path | str,
        device: str | torch.device = "cpu",
        *,
        decoder: bool = True,
    ) -> "Sae":
        path = Path(path)

        with open(path / "cfg.json", "r") as f:
            cfg_dict = json.load(f)
            d_in = cfg_dict.pop("d_in")
            cfg = SaeConfig.from_dict(cfg_dict, drop_extra_fields=True)

        sae = Sae(d_in, cfg, device=device, decoder=decoder)

        state_dict = safe_open(str(path / "sae.safetensors"), framework="torch")
        state_dict = {k: state_dict.get_tensor(k).squeeze() for k in state_dict.keys()}
        sae.load_state_dict(state_dict)

        # load_model(
        #    model=sae,
        #    filename=str(path / "sae.safetensors"),
        #    device=str(device),
        #    # TODO: Maybe be more fine-grained about this in the future?
        #    strict=decoder,
        # )
        return sae

    def save_to_disk(self, path: Path | str):
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)

        save_model(self, str(path / "sae.safetensors"))
        with open(path / "cfg.json", "w") as f:
            json.dump(
                {
                    **self.cfg.to_dict(),
                    "d_in": self.d_in,
                },
                f,
            )

    @property
    def device(self):
        return self.encoder.weight.device

    @property
    def dtype(self):
        return self.encoder.weight.dtype

    def pre_acts(self, x: Tensor) -> Tensor:
        # Remove decoder bias as per Anthropic
        sae_in = x.to(self.dtype) - self.b_dec
        out = self.encoder(sae_in)

        return nn.functional.relu(out)

    def select_topk(self, latents: Tensor) -> torch.return_types.topk:
        """Select the top-k latents."""
        return latents.topk(self.cfg.k, sorted=False)

    def encode(self, x: Tensor) -> EncoderOutput:
        """Encode the input and select the top-k latents."""
        # Encode, decode and compute residual
        feature_acts = self.pre_acts(x)
        if self.cfg.k <= 0:
            if self.jumprelu:
                # JumpReLU SAE
                threshold = torch.exp(self.log_threshold)
                feature_acts = cast(
                    torch.Tensor, JumpReLU.apply(feature_acts, threshold, self.bandwidth)
                )
            top_acts, top_indices = None, None
        else:
            # Top-k SAE
            top_acts, top_indices = self.select_topk(feature_acts)
        return EncoderOutput(feature_acts, top_acts, top_indices)

    def decode(
        self,
        feature_acts: Tensor | None = None,
        top_acts: Tensor | None = None,
        top_indices: Tensor | None = None,
    ) -> Tensor:
        if self.W_dec is None:
            raise RuntimeError("Decoder weight was not initialized.")
        if not ((feature_acts is None) ^ (top_acts is None)):
            raise ValueError("Exactly one of `feature_acts` or `top_acts` should be provided.")
        if top_acts is not None:
            if top_indices is None:
                raise ValueError("`top_indices` must be provided if `top_acts` is provided.")
            y = decoder_impl(top_indices, top_acts.to(self.dtype), self.W_dec.mT)
            return y + self.b_dec
        return feature_acts @ self.W_dec + self.b_dec

    def forward(self, x: Tensor, dead_mask: Tensor | None = None) -> ForwardOutput:
        # Encode, decode and compute residual
        feature_acts, top_acts, top_indices = self.encode(x)
        sae_out = self.decode(feature_acts, top_acts, top_indices)

        # SAE residual
        e = sae_out - x

        # Used as a denominator for putting everything on a reasonable scale
        total_variance = (x - x.mean(0)).pow(2).sum()

        # Second decoder pass for AuxK loss
        if dead_mask is not None and (num_dead := int(dead_mask.sum())) > 0:
            # Heuristic from Appendix B.1 in the paper
            k_aux = x.shape[-1] // 2

            # Reduce the scale of the loss if there are a small number of dead latents
            scale = min(num_dead / k_aux, 1.0)
            k_aux = min(k_aux, num_dead)

            # Don't include living latents in this loss
            auxk_latents = torch.where(dead_mask[None], feature_acts, -torch.inf)

            # Top-k dead latents
            auxk_acts, auxk_indices = auxk_latents.topk(k_aux, sorted=False)

            # Encourage the top ~50% of dead latents to predict the residual of the
            # top k living latents
            e_hat = self.decode(auxk_acts, auxk_indices)
            auxk_loss = (e_hat - e).pow(2).sum()
            auxk_loss = scale * auxk_loss / total_variance
        else:
            auxk_loss = sae_out.new_tensor(0.0)

        l2_loss = (e**2).sum(-1).mean()
        fvu = e.pow(2).sum() / total_variance

        if self.cfg.k > 0 and self.cfg.multi_topk:
            top_acts, top_indices = feature_acts.topk(4 * self.cfg.k, sorted=False)
            sae_out = self.decode(top_acts, top_indices)
            multi_topk_fvu = (sae_out - x).pow(2).sum() / total_variance
        else:
            multi_topk_fvu = sae_out.new_tensor(0.0)

        sparsity_loss = sae_out.new_tensor(0.0)
        if self.cfg.k <= 0:
            if self.jumprelu:
                threshold = torch.exp(self.log_threshold)
                pre_acts_thr = cast(torch.Tensor, Step.apply(feature_acts, threshold, self.bandwidth))  # type: ignore
                sparsity_loss = torch.sum(pre_acts_thr, dim=-1).mean()
            elif self.W_dec is not None:
                # Scale features by the norm of their directions
                weighted_feature_acts = feature_acts * self.W_dec.norm(dim=1)
                sparsity_loss = weighted_feature_acts.norm(p=1, dim=-1).mean()

        return ForwardOutput(
            sae_out,
            feature_acts,
            top_acts,
            top_indices,
            fvu,
            auxk_loss,
            multi_topk_fvu,
            sparsity_loss,
            l2_loss,
        )

    @torch.no_grad()
    def set_decoder_norm_to_unit_norm(self):
        assert self.W_dec is not None, "Decoder weight was not initialized."

        eps = torch.finfo(self.W_dec.dtype).eps
        norm = torch.norm(self.W_dec.data, dim=1, keepdim=True)
        self.W_dec.data /= norm + eps

    @torch.no_grad()
    def remove_gradient_parallel_to_decoder_directions(self):
        assert self.W_dec is not None, "Decoder weight was not initialized."
        assert self.W_dec.grad is not None  # keep pyright happy

        parallel_component = einops.einsum(
            self.W_dec.grad,
            self.W_dec.data,
            "d_sae d_in, d_sae d_in -> d_sae",
        )
        self.W_dec.grad -= einops.einsum(
            parallel_component,
            self.W_dec.data,
            "d_sae, d_sae d_in -> d_sae d_in",
        )
